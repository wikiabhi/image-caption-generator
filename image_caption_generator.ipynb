{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "from os import listdir\n",
    "from pickle import dump, load\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Photo Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Model\n",
    "\n",
    "### About VGG:\n",
    "\n",
    "* Visual Geometry Group from University of Oxford developed VGG model \n",
    "* VGG model won the ImageNet competition in 2014\n",
    "* Published as a conference paper at ICLR 2015: \n",
    "    https://arxiv.org/pdf/1409.1556.pdf\n",
    "* Visual Geometry Group overview: http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "\n",
    "### About VGG16 Model:\n",
    "\n",
    "* 3Ã—3 filters in all convolutional layers\n",
    "* 16 Layers Model\n",
    "* Layer Configurations: https://gist.githubusercontent.com/ksimonyan/211839e770f7b538e2d8/raw/ded9363bd93ec0c770134f4e387d8aaaaa2407ce/VGG_ILSVRC_16_layers_deploy.prototxt\n",
    "\n",
    "### Applications\n",
    "\n",
    "* Given an image, find object name in the image.\n",
    "* It can detect any one of 1000 images.\n",
    "* It takes input image of size 224 * 224 * 3 (RGB image) i.e 224 * 224 pixel image with 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    \n",
    "    # Loading the model\n",
    "    model = VGG16()\n",
    "\n",
    "    # Removing the last layer from the loaded model as we require only the features not the classification \n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "    # Summarizing the model \n",
    "    print(model.summary())\n",
    "\n",
    "    # Extracting features from each photo and storing it in a dictionary \n",
    "    features = dict()\n",
    "\n",
    "    for name in listdir(directory):\n",
    "\n",
    "        # Defining the path of the image \n",
    "        filename = directory + '/' + name\n",
    "        \n",
    "        # Loading an image and converting it into size 224 * 224\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        \n",
    "        # Converting the image pixels into a numpy array\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        # Reshaping data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "        \n",
    "        # Preprocessing the images for the VGG model\n",
    "        # The preprocess_input function is meant to adequate your image to the format the model requires.\n",
    "        image = preprocess_input(image)\n",
    "\n",
    "        # Getting features of an image\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        \n",
    "        # Getting the image name\n",
    "        image_id = name.split('.')[0]\n",
    "\n",
    "        # Storing the feature corresponding to the image in the dictionary\n",
    "        features[image_id] = feature\n",
    "        \n",
    "        # print('>%s' % name)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining the directory we are using\n",
    "directory = 'Flicker8k_Dataset'\n",
    "\n",
    "# Extracting features from all the images\n",
    "features = extract_features(directory)\n",
    "\n",
    "print('Extracted Features: ', len(features))\n",
    "\n",
    "# Dumping the features in a pickle file for further use\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the file containg all the descriptions into memory\n",
    "\n",
    "def load_doc(filename):\n",
    "    # Opening the file as read only\n",
    "    file = open(filename, 'r')\n",
    "\n",
    "    # Reading all text and storing it.\n",
    "    text = file.read()\n",
    "\n",
    "    # Closing the file\n",
    "    file.close()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a dictionary of photo identifiers to the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photo_to_description_mapping(descriptions):\n",
    "    \n",
    "    # Dictionary to store the mapping of photo identifiers to descriptions\n",
    "    description_mapping = dict()\n",
    "    \n",
    "    # Iterating through each line of the descriptions\n",
    "    for line in descriptions.split('\\n'):\n",
    "        \n",
    "        # Splitting the lines by white space\n",
    "        words = line.split()\n",
    "        \n",
    "        # Skipping the lines with length less than 2\n",
    "        if len(line)<2:\n",
    "            continue\n",
    "            \n",
    "        # The first word is the image_id and the rest are the part of the description of that image\n",
    "        image_id, image_description = words[0], words[1:]\n",
    "        \n",
    "        # Retaining only the name of the image and removing the extension from it\n",
    "        image_id = image_id.split('.')[0]\n",
    "        \n",
    "        # Image_descriptions contains comma separated words of the description, hence, converting it back to string\n",
    "        image_description = ' '.join(image_description)\n",
    "        \n",
    "        # There are multiple descriptions per image, \n",
    "        # hence, corresponding to every image identifier in the dictionary, there is a list of description\n",
    "        # if the list does not exist then we need to create it\n",
    "        \n",
    "        if image_id not in description_mapping:\n",
    "            description_mapping[image_id] = list()\n",
    "            \n",
    "        # Now storing the descriptions in the mapping\n",
    "        description_mapping[image_id].append(image_description)\n",
    "    \n",
    "    return description_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to clean the descriptions in the following ways:\n",
    "*     Convert all words to lowercase.\n",
    "*    Remove all punctuation.\n",
    "*     Remove all words that are one character or less in length (e.g. â€˜aâ€™).\n",
    "*     Remove all words with numbers in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(description_mapping):\n",
    "    \n",
    "    # Preapring a translation table for removing all the punctuation\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    \n",
    "    # Traversing through the mapping we created\n",
    "    for key, descriptions in description_mapping.items():\n",
    "        for i in range(len(descriptions)):\n",
    "            description = descriptions[i]\n",
    "            description = description.split()\n",
    "            \n",
    "            # Converting all the words to lower case\n",
    "            description = [word.lower() for word in description]\n",
    "            \n",
    "            # Removing the punctuation using the translation table we made\n",
    "            description = [word.translate(table) for word in description]\n",
    "            \n",
    "            # Removing the words with length =1\n",
    "            description = [word for word in description if len(word)>1]\n",
    "            \n",
    "            # Removing all words with number in them\n",
    "            description = [word for word in description if word.isalpha()]\n",
    "            \n",
    "            # Converting the description back to string and overwriting in the descriptions list\n",
    "            descriptions[i] = ' '.join(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we want a vocabulary that is both expressive and as small as possible. A smaller vocabulary will result in a smaller model that will train faster.\n",
    "\n",
    "For reference, we can transform the clean descriptions into a set and print its size to get an idea of the size of our dataset vocabulary.\n",
    "\n",
    "Sets are highly optimized, don't contain any duplicate values. There implementation is based on hash table. Hence we get a vocabulary that is both expressive and small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the loaded descriptions into a vocabulary of words\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    \n",
    "    # Build a list of all description strings\n",
    "    all_desc = set()\n",
    "    \n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    \n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "# Loading descriptions\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# Parsing descriptions\n",
    "descriptions = photo_to_description_mapping(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "\n",
    "# Cleaning the descriptions\n",
    "clean_descriptions(descriptions)\n",
    "\n",
    "# Summarizing the vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "\n",
    "# Saving to the file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Developing Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
